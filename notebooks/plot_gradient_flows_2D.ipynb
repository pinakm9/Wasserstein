{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Gradient flows in 2D\n",
    "====================\n",
    "\n",
    "Let's showcase the properties of **kernel MMDs**, **Hausdorff**\n",
    "and **Sinkhorn** divergences on a simple toy problem:\n",
    "the registration of one blob onto another.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup\n",
    "---------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from geomloss import SamplesLoss\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "dtype    = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display routine\n",
    "~~~~~~~~~~~~~~~~~\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imageio'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a38d38171645>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mchoices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mimageio\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimread\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'imageio'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from random import choices\n",
    "from imageio import imread\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def load_image(fname) :\n",
    "    img = imread(fname, as_gray = True) # Grayscale\n",
    "    img = (img[::-1, :])  / 255.\n",
    "    return 1 - img\n",
    "\n",
    "def draw_samples(fname, n, dtype=torch.FloatTensor) :\n",
    "    A = load_image(fname)\n",
    "    xg, yg = np.meshgrid( np.linspace(0,1,A.shape[0]), np.linspace(0,1,A.shape[1]) )\n",
    "    \n",
    "    grid = list( zip(xg.ravel(), yg.ravel()) )\n",
    "    dens = A.ravel() / A.sum()\n",
    "    dots = np.array( choices(grid, dens, k=n ) )\n",
    "    dots += (.5/A.shape[0]) * np.random.standard_normal(dots.shape)\n",
    "\n",
    "    return torch.from_numpy(dots).type(dtype)\n",
    "\n",
    "def display_samples(ax, x, color) :\n",
    "    x_ = x.detach().cpu().numpy()\n",
    "    ax.scatter( x_[:,0], x_[:,1], 25*500 / len(x_), color, edgecolors='none' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset\n",
    "~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "Our source and target samples are drawn from intervals of the real line\n",
    "and define discrete probability measures:\n",
    "\n",
    "\\begin{align}\\alpha ~=~ \\frac{1}{N}\\sum_{i=1}^N \\delta_{x_i}, ~~~\n",
    "  \\beta  ~=~ \\frac{1}{M}\\sum_{j=1}^M \\delta_{y_j}.\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, M = (100, 100) if not use_cuda else (10000, 10000)\n",
    " \n",
    "X_i = draw_samples(\"data/density_a.png\", N, dtype)\n",
    "Y_j = draw_samples(\"data/density_b.png\", M, dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wasserstein gradient flow\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "To study the influence of the $\\text{Loss}$ function in measure-fitting\n",
    "applications, we perform gradient descent on the positions\n",
    "$x_i$ of the samples that make up $\\alpha$\n",
    "as we minimize the cost $\\text{Loss}(\\alpha,\\beta)$.\n",
    "This procedure can be understood as a discrete (Lagrangian) \n",
    "`Wasserstein gradient flow <https://arxiv.org/abs/1609.03890>`_\n",
    "and as a \"model-free\" machine learning program, where\n",
    "we optimize directly on the samples' locations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_flow(loss, lr=.05) :\n",
    "    \"\"\"Flows along the gradient of the cost function, using a simple Euler scheme.\n",
    "    \n",
    "    Parameters:\n",
    "        loss ((x_i,y_j) -> torch float number): \n",
    "            Real-valued loss function.\n",
    "        lr (float, default = .05):\n",
    "            Learning rate, i.e. time step.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parameters for the gradient descent\n",
    "    Nsteps = int(5/lr)+1 \n",
    "    display_its = [int(t/lr) for t in [0, .25, .50, 1., 2., 5.]]\n",
    "    \n",
    "    # Use colors to identify the particles\n",
    "    colors = (10*X_i[:,0]).cos() * (10*X_i[:,1]).cos()\n",
    "    colors = colors.detach().cpu().numpy()\n",
    "    \n",
    "    # Make sure that we won't modify the reference samples\n",
    "    x_i, y_j = X_i.clone(), Y_j.clone()\n",
    "\n",
    "    # We're going to perform gradient descent on Loss(α, β) \n",
    "    # wrt. the positions x_i of the diracs masses that make up α:\n",
    "    x_i.requires_grad = True  \n",
    "    \n",
    "    t_0 = time.time()\n",
    "    plt.figure(figsize=(12,8)) ; k = 1\n",
    "    for i in range(Nsteps): # Euler scheme ===============\n",
    "        # Compute cost and gradient\n",
    "        L_αβ = loss(x_i, y_j)\n",
    "        [g]  = torch.autograd.grad(L_αβ, [x_i])\n",
    "\n",
    "        if i in display_its : # display\n",
    "            ax = plt.subplot(2,3,k) ; k = k+1\n",
    "            plt.set_cmap(\"hsv\")\n",
    "            plt.scatter( [10], [10] ) # shameless hack to prevent a slight change of axis...\n",
    "\n",
    "            display_samples(ax, y_j, [(.55,.55,.95)])\n",
    "            display_samples(ax, x_i, colors)\n",
    "            \n",
    "            ax.set_title(\"t = {:1.2f}\".format(lr*i))\n",
    "\n",
    "            plt.axis([0,1,0,1])\n",
    "            plt.gca().set_aspect('equal', adjustable='box')\n",
    "            plt.xticks([], []); plt.yticks([], [])\n",
    "            plt.tight_layout()\n",
    "        \n",
    "        # in-place modification of the tensor's values\n",
    "        x_i.data -= lr * len(x_i) * g \n",
    "    plt.title(\"t = {:1.2f}, elapsed time: {:.2f}s/it\".format(lr*i, (time.time() - t_0)/Nsteps ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel norms, MMDs\n",
    "------------------------------------\n",
    "\n",
    "Gaussian MMD\n",
    "~~~~~~~~~~~~~~~\n",
    "\n",
    "The smooth Gaussian kernel\n",
    "$k(x,y) = \\exp(-\\|x-y\\|^2/2\\sigma^2)$\n",
    "is blind to details which are smaller than the blurring scale $\\sigma$:\n",
    "its gradient stops being informative when $\\alpha$\n",
    "and $\\beta$ become equal \"up to the high frequencies\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_flow( SamplesLoss(\"gaussian\", blur=.5) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, if the radius $\\sigma$\n",
    "of the kernel is too small, particles $x_i$\n",
    "won't be attracted to the target, and may **spread out**\n",
    "to minimize the auto-correlation term \n",
    "$\\tfrac{1}{2}\\langle \\alpha, k\\star\\alpha\\rangle$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_flow( SamplesLoss(\"gaussian\", blur=.1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laplacian MMD\n",
    "~~~~~~~~~~~~~~~~\n",
    "\n",
    "The pointy exponential kernel\n",
    "$k(x,y) = \\exp(-\\|x-y\\|/\\sigma)$\n",
    "tends to provide a better fit, but tends to zero at infinity\n",
    "and is still very prone to **screening artifacts**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_flow( SamplesLoss(\"laplacian\", blur=.1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Energy Distance MMD\n",
    "~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "The scale-equivariant kernel\n",
    "$k(x,y)=-\\|x-y\\|$ provides a robust baseline:\n",
    "the Energy Distance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gradient_flow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-8ab5c64a8bab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# sphinx_gallery_thumbnail_number = 4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgradient_flow\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mSamplesLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"energy\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'gradient_flow' is not defined"
     ]
    }
   ],
   "source": [
    "# sphinx_gallery_thumbnail_number = 4\n",
    "gradient_flow( SamplesLoss(\"energy\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sinkhorn divergence\n",
    "----------------------\n",
    "\n",
    "(Unbiased) Sinkhorn divergences have recently been\n",
    "introduced in the machine learning litterature,\n",
    "and can be understood as modern iterations\n",
    "of the classic `SoftAssign <https://en.wikipedia.org/wiki/Point_set_registration#Robust_point_matching>`_ algorithm\n",
    "from `economics <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.228.9750&rep=rep1&type=pdf>`_ and \n",
    "`computer vision <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.86.9769&rep=rep1&type=pdf>`_.\n",
    "\n",
    "\n",
    "Wasserstein-1 distance\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "When ``p = 1``, the Sinkhorn divergence $\\text{S}_\\varepsilon$\n",
    "interpolates between the Energy Distance (when $\\varepsilon$ is large):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_flow( SamplesLoss(\"sinkhorn\", p=1, blur=1.) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the Earth-Mover's (Wassertein-1) distance:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_flow( SamplesLoss(\"sinkhorn\", p=1, blur=.01) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wasserstein-2 distance\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "When ``p = 2``, $\\text{S}_\\varepsilon$\n",
    "interpolates between the degenerate kernel norm\n",
    "\n",
    "\\begin{align}\\tfrac{1}{2}\\| \\alpha-\\beta\\|^2_{-\\tfrac{1}{2}\\|\\cdot\\|^2}\n",
    "  ~=~ \\tfrac{1}{2}\\| \\int x \\text{d}\\alpha(x)~-~\\int y \\text{d}\\beta(y)\\|^2,\\end{align}\n",
    "\n",
    "which only registers the means of both measures with each other \n",
    "(when $\\varepsilon$ is large):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_flow( SamplesLoss(\"sinkhorn\", p=2, blur=1.) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the quadratic, Wasserstein-2 Optimal Transport\n",
    "distance which has been studied so well by mathematicians\n",
    "from the 80's onwards (when $\\varepsilon$ is small):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_flow( SamplesLoss(\"sinkhorn\", p=2, blur=.01) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduced in 2016-2018, the *unbalanced*\n",
    "setting (Gaussian-Hellinger, Wasserstein-Fisher-Rao, etc.)\n",
    "provides a principled way of introducing a **threshold**\n",
    "in Optimal Transport computations:\n",
    "it allows you to introduce **laziness** in the transportation problem\n",
    "by replacing distance fields $\\|x-y\\|$\n",
    "with a robustified analogous $\\rho\\cdot( 1 - e^{-\\|x-y\\|/\\rho} )$,\n",
    "whose gradient saturates beyond a given **reach**, $\\rho$\n",
    "- at least, that's the idea.\n",
    "\n",
    "In real-life applications, this tunable parameter could allow\n",
    "you to be a little bit more **robust to outliers**!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_flow( SamplesLoss(\"sinkhorn\", p=2, blur=.01, reach=.3) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
