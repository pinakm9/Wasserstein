{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4) Sinkhorn vs. blurred Wasserstein distances\n",
    "==========================================================\n",
    "\n",
    "Sinkhorn divergences rely on a simple idea:\n",
    "by **blurring** the transport plan through the addition of\n",
    "an entropic penalty, we can reduce the effective dimensionality\n",
    "of the transportation problem and compute **sensible approximations of the\n",
    "Wasserstein distance at a low computational cost**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in previous notebooks, the *vanilla* Sinkhorn loop\n",
    "can be symmetrized, de-biased and turned into a genuine\n",
    "multiscale algorithm: available through the\n",
    ":mod:`SamplesLoss(\"sinkhorn\") <geomloss.SamplesLoss>` layer, the **Sinkhorn divergence** \n",
    "\n",
    "\\begin{align}\\text{S}_\\varepsilon(\\alpha,\\beta)~=~ \\text{OT}_\\varepsilon(\\alpha,\\beta)\n",
    "  - \\tfrac{1}{2}\\text{OT}_\\varepsilon(\\alpha,\\alpha)\n",
    "  - \\tfrac{1}{2}\\text{OT}_\\varepsilon(\\beta,\\beta),\\end{align}\n",
    "\n",
    "is a tractable approximation of the Wasserstein distance\n",
    "that **retains its key geometric properties** - positivity, convexity,\n",
    "metrization of the convergence in law.\n",
    "\n",
    "**But is it really the best way of smoothing our transportation problem?**\n",
    "When \"p = 2\" and $\\text{C}(x,y)=\\tfrac{1}{2}\\|x-y\\|^2$,\n",
    "a very sensible alternative to Sinkhorn divergences is the\n",
    "**blurred Wasserstein distance**\n",
    "\n",
    "\\begin{align}\\text{B}_\\varepsilon(\\alpha,\\beta) ~=~ \\text{W}_2(\\,k_{\\varepsilon/4}\\star\\alpha,\\,k_{\\varepsilon/4}\\star\\beta\\,),\\end{align}\n",
    "\n",
    "where $\\text{W}_2$ denotes the *true* Wasserstein distance associated to\n",
    "our cost function $\\text{C}$ and\n",
    "\n",
    "\\begin{align}k_{\\varepsilon/4}: (x-y) \\mapsto \\exp(-\\|x-y\\|^2 / \\tfrac{2}{4}\\varepsilon)\\end{align}\n",
    "\n",
    "is a Gaussian kernel of deviation $\\sigma = \\sqrt{\\varepsilon}/2$.\n",
    "On top of making explicit our intuitions on **low-frequency Optimal Transport**, this\n",
    "simple divergence enjoys a collection of desirable properties:\n",
    "\n",
    "- It is the **square of a distance** that metrizes the convergence in law.\n",
    "- It takes the \"correct\" values on atomic **Dirac masses**, lifting\n",
    "  the ground cost function to the space of positive measures:\n",
    "\n",
    "  .. math::\n",
    "    \\text{B}_\\varepsilon(\\delta_x,\\delta_y)~=~\\text{C}(x,y)\n",
    "    ~=~\\tfrac{1}{2}\\|x-y\\|^2~=~\\text{S}_\\varepsilon(\\delta_x,\\delta_y).\n",
    "\n",
    "- It has the same **asymptotic properties** as the Sinkhorn divergence,\n",
    "  interpolating between the true Wasserstein distance (when $\\varepsilon \\rightarrow 0$)\n",
    "  and a degenerate kernel norm (when $\\varepsilon \\rightarrow +\\infty$).\n",
    "- Thanks to the joint convexity of the Wasserstein distance, \n",
    "  $\\text{B}_\\varepsilon(\\alpha,\\beta)$ is a **decreasing** function of $\\varepsilon$:\n",
    "  as we remove small-scale details, we lower the overall transport cost.\n",
    "\n",
    "To compare the Sinkhorn and blurred Wasserstein divergences, a simple experiment\n",
    "is to **display their values on pairs of 1D measures** for increasing values of \n",
    "the temperature $\\varepsilon$:\n",
    "having generated random samples $\\alpha$ and $\\beta$ \n",
    "on the unit interval, we can simply compute $\\text{S}_\\varepsilon(\\alpha,\\beta)$\n",
    "with our :mod:`SamplesLoss(\"sinkhorn\") <geomloss.SamplesLoss>` layer\n",
    "while the blurred Wasserstein loss $\\text{B}_\\varepsilon(\\alpha,\\beta)$ can be\n",
    "quickly approximated with the **addition of a Gaussian noise** followed\n",
    "by a **sorting pass**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup\n",
    "---------------------\n",
    "Standard imports:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pyKeOps]: Warning, no cuda detected. Switching to cpu only.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KernelDensity  # display as density curves\n",
    "\n",
    "import torch\n",
    "from geomloss import SamplesLoss\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# N.B.: We use float64 numbers to get nice limits when blur -> +infinity\n",
    "dtype    = torch.cuda.DoubleTensor if use_cuda else torch.DoubleTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display routine:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_plot = np.linspace(-0.5, 1.5, 1000)[:,np.newaxis]\n",
    "\n",
    "def display_samples(ax, x, color, label=None):\n",
    "    \"\"\"Displays samples on the unit interval using a density curve.\"\"\"\n",
    "    kde  = KernelDensity(kernel='gaussian', bandwidth= .005 ).fit(x.data.cpu().numpy())\n",
    "    dens = np.exp( kde.score_samples(t_plot) )\n",
    "    dens[0] = 0 ; dens[-1] = 0\n",
    "    ax.fill(t_plot, dens, color=color, label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment\n",
    "-------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generic_logsumexp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-7d59c90ee0be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m#       with a very large 'scaling' coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSamplesLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sinkhorn\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblur\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mblur\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaling\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.99\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"online\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0msink\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_j\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# Compute the blurred Wasserstein distance:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pinak\\.conda\\envs\\py38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pinak\\.conda\\envs\\py38\\lib\\site-packages\\geomloss\\samples_loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[1;31m# Run --------------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m         values = routines[self.loss][backend]( α, x, β, y, \n\u001b[0m\u001b[0;32m    232\u001b[0m                     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblur\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblur\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreach\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreach\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m                     \u001b[0mdiameter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiameter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaling\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscaling\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pinak\\.conda\\envs\\py38\\lib\\site-packages\\geomloss\\sinkhorn_samples.py\u001b[0m in \u001b[0;36msinkhorn_online\u001b[1;34m(α, x, β, y, p, blur, reach, diameter, scaling, cost, debias, potentials, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcost\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcost_formulas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m     \u001b[0msoftmin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoftmin_online\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_conv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeops_lse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;31m# The \"cost matrices\" are implicitely encoded in the point clouds,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pinak\\.conda\\envs\\py38\\lib\\site-packages\\geomloss\\sinkhorn_samples.py\u001b[0m in \u001b[0;36mkeops_lse\u001b[1;34m(cost, D, dtype)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mkeops_lse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"float32\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     log_conv = generic_logsumexp(\"( B - (P * \" + cost + \" ) )\",\n\u001b[0m\u001b[0;32m     74\u001b[0m                                  \u001b[1;34m\"A = Vi(1)\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                                  \u001b[1;34m\"X = Vi({})\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'generic_logsumexp' is not defined"
     ]
    }
   ],
   "source": [
    "def rweight():\n",
    "    \"\"\"Random weight.\"\"\"\n",
    "    return torch.rand(1).type(dtype)\n",
    "\n",
    "N = 100 if not use_cuda else 10**3  # Number of samples per measure\n",
    "C = 100 if not use_cuda else 10000  # number of copies for the Gaussian blur\n",
    "\n",
    "for _ in range(5):  # Repeat the experiment 5 times\n",
    "    K = 5  # Generate random 1D measures as the superposition of K=5 intervals\n",
    "    t = torch.linspace(0, 1, N//K).type(dtype).view(-1,1)\n",
    "    X_i = torch.cat( [ rweight()**2 * t + rweight() - .5 for k in range(K)], dim=0 )\n",
    "    Y_j = torch.cat( [ rweight()**2 * t + rweight() - .5 for k in range(K)], dim=0 )\n",
    "\n",
    "    # Compute the limits when blur = 0...\n",
    "    x_, _ = X_i.sort(dim=0) ; y_, _ = Y_j.sort(dim=0)\n",
    "    true_wass = (.5 / len(X_i)) * ((x_ - y_)**2).sum()\n",
    "    # and when blur = +infinity:\n",
    "    mean_diff = .5 * (( X_i.mean(0) - Y_j.mean(0) )**2).sum()\n",
    "    \n",
    "    blurs = [.01, .02, .05, .1, .2, .5, 1., 2., 5., 10.]\n",
    "    sink, bwass = [], []\n",
    "\n",
    "    for blur in blurs:\n",
    "        # Compute the Sinkhorn divergence:\n",
    "        # N.B.: To be super-precise, we use the well-tested \"online\" backend\n",
    "        #       with a very large 'scaling' coefficient\n",
    "        loss = SamplesLoss(\"sinkhorn\", p=2, blur=blur, scaling=.99, backend=\"online\")\n",
    "        sink.append( loss(X_i, Y_j).item() )\n",
    "\n",
    "        # Compute the blurred Wasserstein distance:\n",
    "        x_i = torch.cat( [X_i] * C, dim=0 )\n",
    "        y_j = torch.cat( [Y_j] * C, dim=0 )\n",
    "        x_i = x_i + .5 * blur * torch.randn(x_i.shape).type(dtype)\n",
    "        y_j = y_j + .5 * blur * torch.randn(y_j.shape).type(dtype)\n",
    "        x_, _ = x_i.sort(dim=0) ; y_, _ = y_j.sort(dim=0)\n",
    "\n",
    "        wass = (.5 / len(x_i)) * ((x_ - y_)**2).sum()\n",
    "        bwass.append(wass.item())\n",
    "\n",
    "    # Fancy display:\n",
    "    plt.figure(figsize=(12,5))\n",
    "\n",
    "    if N < 10**5:\n",
    "        ax = plt.subplot(1,2,1)\n",
    "        display_samples(ax, X_i, (1.,0,0,.5), label=\"$\\\\alpha$\")\n",
    "        display_samples(ax, Y_j, (0,0,1.,.5), label=\"$\\\\beta$\")\n",
    "        plt.axis([-.5,1.5,-.1,5.5])\n",
    "        plt.ylabel(\"density\") ; ax.legend()\n",
    "        plt.tight_layout()\n",
    "\n",
    "    ax = plt.subplot(1,2,2)\n",
    "    plt.plot([.01,10], [true_wass, true_wass], \"g\", label=\"True Wasserstein\")\n",
    "    plt.plot(blurs, sink,  \"r-o\", label=\"Sinkhorn divergence\")\n",
    "    plt.plot(blurs, bwass, \"b-o\", label=\"Blurred Wasserstein\")\n",
    "    plt.plot([.01,10], [mean_diff, mean_diff], \"m\", label=\"Squared difference of means\")\n",
    "    ax.set_xscale(\"log\") ; ax.legend()\n",
    "    plt.axis([.01, 10., 0., 1.5 * bwass[0]])\n",
    "    plt.xlabel(\"blur $\\\\sqrt{\\\\varepsilon}$\")\n",
    "    plt.tight_layout() ; plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "--------------\n",
    "\n",
    "In practice, the Sinkhorn and blurred Wasserstein divergences\n",
    "are **nearly indistinguishable**. But as far as we can tell *today*,\n",
    "these two loss functions have very different properties:\n",
    "\n",
    "- $\\text{B}_\\varepsilon$ is **easy to define**, compute in 1D and\n",
    "  **analyze** from geometric or statistical point of views... \n",
    "  But cannot (?) be computed efficiently in higher dimensions,\n",
    "  where the true OT problem is nearly intractable.\n",
    "- $\\text{S}_\\varepsilon$ is simply available through\n",
    "  the :mod:`SamplesLoss(\"sinkhorn\") <geomloss.SamplesLoss>` layer,\n",
    "  but has a weird, composite definition and is pretty **hard to**\n",
    "  **study** rigorously - as evidenced by recent, technical proofs\n",
    "  of `positivity, definiteness (Feydy et al., 2018) <https://arxiv.org/abs/1810.08278>`_  \n",
    "  and `sample complexity (Genevay et al., 2018) <https://arxiv.org/abs/1810.02733>`_.\n",
    "\n",
    "**So couldn't we get the best of both worlds?**\n",
    "In an ideal world, we'd like to tweak the *efficient* multiscale Sinkhorn algorithm\n",
    "to compute the *natural* divergence $\\text{B}_\\varepsilon$...\n",
    "but this may be out of reach. A realistic target could be to **quantify**\n",
    "**the difference** between these two objects, thus legitimizing the\n",
    "use of the :mod:`SamplesLoss(\"sinkhorn\") <geomloss.SamplesLoss>` layer\n",
    "as a **cheap proxy** for the intuitive and well-understood *blurred Wasserstein distance*.\n",
    "\n",
    "In my opinion, investigating the link between these two quantities\n",
    "is one of the most interesting questions left open in the field of discrete entropic OT.\n",
    "The geometric loss functions implemented in GeomLoss are probably *good enough*\n",
    "for most practical purposes,\n",
    "but getting a **rigorous understanding** of the multiscale, \n",
    "wavelet-like behavior of our algorithms\n",
    "as we add small details through an exponential decay of \n",
    "the blurring scale $\\sqrt{\\varepsilon}$ would be truly insightful. \n",
    "In some sense, couldn't we prove a \n",
    "`Hilbert <https://en.wikipedia.org/wiki/Orthonormal_basis>`_-`Plancherel <https://en.wikipedia.org/wiki/Plancherel_theorem>`_\n",
    "theorem for the Wasserstein distance?\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
